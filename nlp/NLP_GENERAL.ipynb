{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"
      ],
      "metadata": {
        "id": "z9gxKVqJZ9dD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏"
      ],
      "metadata": {
        "id": "L63cLUzEaKXZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vja8TF_MRAw2"
      },
      "outputs": [],
      "source": [
        "!pip install pymorphy2\n",
        "!pip install ufal.udpipe\n",
        "!pip install wget\n",
        "!pip install gensim\n",
        "!pip install umap-learn\n",
        "!pip install datashader\n",
        "!pip install bokeh\n",
        "!pip install holoviews\n",
        "!pip install yargy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd                                   # –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏\n",
        "import seaborn as sns                                 # –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "import pymorphy2 as mph                               # –î–ª—è –ª–µ–º–º–µ—Ç–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
        "import re                                             # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è\n",
        "import wget                                           # –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤\n",
        "import sys                                            # –î–ª—è –∏—Å–ø–æ–ª—å–Ω–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –∫–æ–º–∞–Ω–¥\n",
        "from gensim.models import Word2Vec as w2v             # –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Word2vec\n",
        "import logging                                        # –î–ª—è –≤–≤–µ–¥–µ–Ω–∏—è –ª–æ–≥–æ–≤\n",
        "import string\n",
        "import nltk\n",
        "from nltk import word_tokenize                        # –î–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —Ç–æ–∫–µ–Ω—ã\n",
        "from nltk.corpus import stopwords                     # –î–ª—è —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
        "import random                                         # –î–ª—è –ø–µ—Ä–µ–º–µ—â–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
        "import json                                           # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞—Å—Å–∏–≤–∞\n",
        "import numpy as np                                    # –î–ª—è –ª–∏–Ω–∞–ª–∞\n",
        "import umap                                           # –î–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≤ –¥–≤—É—Ö–º–µ—Ä–Ω–æ–µ\n",
        "import matplotlib.pyplot as plt                       # –î–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from yargy import Parser, rule, and_, or_             # –ü–∞—Ä—Å–µ—Ä\n",
        "from yargy.interpretation import fact, attribute      # –ü–∞—Ä—Å–µ—Ä\n",
        "from yargy.predicates import normalized, dictionary   # –ü–∞—Ä—Å–µ—Ä\n",
        "from yargy.pipelines import morph_pipeline            # –ü–∞—Ä—Å–µ—Ä\n",
        "from yargy.relations import main                      # –ü–∞—Ä—Å–µ—Ä\n",
        "from IPython.display import display                   # –ü–∞—Ä—Å–µ—Ä\n",
        "import spacy                                          # –ü–∞—Ä—Å–µ—Ä"
      ],
      "metadata": {
        "id": "nxYwSeyUaOLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "sw = stopwords.words('russian')"
      ],
      "metadata": {
        "id": "cUWebPm8aTn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
        "\n",
        "## 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
        "\n",
        "\n",
        "* 1. ([Kaggle](https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing)).\n",
        "* 2. (https://www.kaggle.com/code/abdmental01/text-preprocessing-nlp-steps-to-process-text)).\n",
        "* 3. (https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)"
      ],
      "metadata": {
        "id": "TNjBc2VEaaet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "svNVtL3eaiuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patterns = \"[A-Za-z0-9!#$%&'()*+/:;<=>?@[\\]^_`{|}~‚Äî\\\"]+\"\n",
        "morph = mph.MorphAnalyzer()\n",
        "\n",
        "def lemmatize(doc):\n",
        "    doc = re.sub(patterns, ' ', doc)\n",
        "    tokens = []\n",
        "    for token in doc.split():\n",
        "        if token:\n",
        "            token = token.strip()\n",
        "            token = morph.normal_forms(token)[0]\n",
        "            tokens.append(token)\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "TkgngBi7aZP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ù–∞—Ç–∞—à–∞\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9F5OUFa1ak3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_name = []\n",
        "topic_one_to_one = []\n",
        "Case = fact('Case', ['name'])\n",
        "\n",
        "def make_topic(topic: list, name: str):\n",
        "    global topic_name\n",
        "\n",
        "    topic_name.append(morph_pipeline(topic).interpretation(\n",
        "          Case.name.const(name)\n",
        "      ).interpretation(\n",
        "          Case\n",
        "      )\n",
        "    )\n",
        "\n",
        "def make_topic_one_to_one(topic: list):\n",
        "    global topic_name\n",
        "\n",
        "    return morph_pipeline(topic).interpretation(\n",
        "          Case.name.normalized()\n",
        "      ).interpretation(\n",
        "          Case\n",
        "      )\n",
        "\n",
        "top_topic = [\n",
        "    ([\"–æ–∫—Ä—É–∂–Ω–æ—Å—Ç—å\", \"—É–≥–æ–ª\"], '–ì–µ–æ–º–µ—Ç—Ä–∏—è'),\n",
        "\n",
        "    ([\"–¥–µ–ª–µ–Ω–∏–µ\", \"–º–Ω–æ–∂–∏—Ç–µ–ª–∏\"], '–ú–Ω–æ–≥–æ—á–ª–µ–Ω'),\n",
        "\n",
        "    ([\"–∫–ª–µ—Ç–∫–∞\", \"–∑–∞–∫—Ä–∞—Å–∏—Ç—å\"], '–î–∏—Ä–∏—Ö–ª–µ'),\n",
        "\n",
        "    ([\"–¥–µ–ª–∏—Ç—Å—è\", \"–æ–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è\"], '–¢–µ–æ—Ä–∏—è —á–∏—Å–µ–ª'),\n",
        "\n",
        "    ([\"—Å–ø–æ—Å–æ–±\", \"—Ä–∞–∑–¥–µ–ª–∏—Ç—å\"], '–ö–æ–º–±–∏–Ω–∞—Ç–æ—Ä–∏–∫–∞'),\n",
        "\n",
        "    ([\"–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\", \"—Ä–∞–∑—Ä–µ—à–∞—Ç—å—Å—è\"], '–ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç'),\n",
        "\n",
        "    ([\"—Å—É–º–º–∞\", \"–∫–∞–∂–¥—ã–π\", ], '–û—Ü–µ–Ω–∫–∞+–ü—Ä–∏–º–µ—Ä'),\n",
        "\n",
        "    (['–≥–æ—Ä–æ–¥', \"—Ä–µ–±—Ä–æ\",], '–ì—Ä–∞—Ñ—ã')\n",
        "]\n",
        "\n",
        "for name_complaint in top_topic:\n",
        "    make_topic(name_complaint[0], name_complaint[1])\n",
        "    topic_one_to_one.extend(list(name_complaint[0]))\n",
        "    for columns in list(name_complaint[0]):\n",
        "      data[columns] = np.NaN\n",
        "\n",
        "OTHERS = make_topic_one_to_one(topic_one_to_one)\n",
        "\n",
        "ALL = or_(*topic_name).interpretation(Case)\n",
        "OTHERS_ALL = or_(OTHERS).interpretation(Case)"
      ],
      "metadata": {
        "id": "_KZBPg6pbC7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%time\n",
        "parser = Parser(OTHERS_ALL)\n",
        "for ind, elem in enumerate(data['task']):\n",
        "    for match in parser.findall(str(elem)):\n",
        "        data.loc[ind, match.fact.name] = 1\n",
        "\n",
        "parser = Parser(ALL)\n",
        "for ind, elem in enumerate(data['task']):\n",
        "    for match in parser.findall(str(elem)):\n",
        "        data.loc[ind, match.fact.name] = 1"
      ],
      "metadata": {
        "id": "ddLRaC-qay5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–°—Ç–æ–ø —Å–ª–æ–≤–∞"
      ],
      "metadata": {
        "id": "Mzea4BeubJv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
        "def remove_stopwords(lines, sw=sw):\n",
        "    res = []\n",
        "    for line in lines:\n",
        "        original = line\n",
        "        line = [w for w in line if w not in sw]\n",
        "        if len(line) < 1:\n",
        "            line = original\n",
        "        res.append(line)\n",
        "    return res\n",
        "\n",
        "%time filtered_lines = remove_stopwords(lines=lines, sw=sw)"
      ],
      "metadata": {
        "id": "FhyzB4ozbKoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec"
      ],
      "metadata": {
        "id": "K9T6QZfwbO5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –ü–µ—Ä–µ–º–µ—â–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫\n",
        "random.shuffle(filtered_lines)\n",
        "# –û–±—É—á–∞–µ–º word2vec\n",
        "%time model = w2v(filtered_lines, min_count=3, sg=1, window=7)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\n",
        "model.save(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "hxsfv6hybQn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "model = w2v.load(\"/content/drive/MyDrive/–ü—Ä–æ–µ–∫—Ç—ã/Medsi/Models/word2vec.model\")\n",
        "\n",
        "# –ü—Ä–æ–∏–∑–≤–æ–¥–∏–º –ª–µ–º–º–∏—Ç–∏–∑–∞—Ü–∏—é –∫–æ–ª–æ–∫–Ω–∏\n",
        "merge_data_filter_2.illness_hostory = merge_data_filter_2.illness_hostory.apply(lemmatize)\n",
        "\n",
        "# –í–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä—É–µ–º\n",
        "for i in range(100):\n",
        "  merge_data_filter_2[f'vector_{i}'] = 0\n",
        "\n",
        "for j, text in enumerate(merge_data_filter_2['illness_hostory']):\n",
        "  vec = np.zeros(100)\n",
        "  lens = 0\n",
        "  for word in word_tokenize(text):\n",
        "      try:\n",
        "        vec += model.wv[word]\n",
        "        lens += 1\n",
        "      except KeyError:\n",
        "        continue\n",
        "\n",
        "  vec /= lens\n",
        "  for i in range(100):\n",
        "    merge_data_filter_2.iloc[j, 103+i] = vec[i]"
      ],
      "metadata": {
        "id": "6QztLyJabVLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Umap"
      ],
      "metadata": {
        "id": "8ZaRCgLRbcpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import umap.plot"
      ],
      "metadata": {
        "id": "MTK6Cp0Dbf5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapper = umap.UMAP(densmap=True).fit(X)\n",
        "umap.plot.points(mapper)"
      ],
      "metadata": {
        "id": "9Stkupgnberf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏"
      ],
      "metadata": {
        "id": "JcIy_pGMb6qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "CGh5ZySab9SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–û–±–ª–∞–∫–æ —Å–ª–æ–≤"
      ],
      "metadata": {
        "id": "pYc6eamsb-Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "y7GLBiU5cJ6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic in data.topic.unique():\n",
        "    df = data[data.topic == topic]\n",
        "    text = ' '.join(df['new_task'])\n",
        "    text_tokens = word_tokenize(text)\n",
        "\n",
        "    cloud = WordCloud(stopwords=stop_words,\n",
        "                      background_color='white').generate(' '.join(text_tokens))\n",
        "    plt.imshow(cloud)\n",
        "    plt.axis('off')\n",
        "    plt.title(topic)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lKoN4csJcFOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-–≥—Ä–∞–º–º—ã"
      ],
      "metadata": {
        "id": "h1ZRmWGrcOwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 30\n",
        "n = 2\n",
        "for topic in data.topic.unique():\n",
        "    df = data[data.topic == topic]\n",
        "    words = ' '.join(df.new_task_pros)\n",
        "    words = ' '.join(list(filter(lambda x: len(x) >= 2, (words.split()))))\n",
        "    tokens = nltk.word_tokenize(words)\n",
        "\n",
        "    ngrams_list = list(ngrams(tokens, n))\n",
        "    freq_dist = dict(FreqDist(ngrams_list))\n",
        "    sorted_data = sorted(freq_dist.items(), key=lambda x: -x[1])\n",
        "\n",
        "    y_labels = [str(key) for key, _ in sorted_data][:k][::-1]\n",
        "    x_values = [value for _, value in sorted_data][:k][::-1]\n",
        "\n",
        "    plt.barh(y_labels, x_values)\n",
        "    plt.xlabel('–ó–Ω–∞—á–µ–Ω–∏–µ')\n",
        "    plt.ylabel('–ö–æ—Ä—Ç–µ–∂–∏')\n",
        "    plt.title(topic)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "3-6FC_GzcQVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF"
      ],
      "metadata": {
        "id": "4IemS0spcbfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vect_tfidf(text):\n",
        "  return vectorizer.transform([text]).toarray()"
      ],
      "metadata": {
        "id": "7eYz6xlxcb4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features=5000, min_df=3)\n",
        "X = vectorizer.fit_transform(learn_tf_idf)"
      ],
      "metadata": {
        "id": "ygjAv4V7chU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenserflow token"
      ],
      "metadata": {
        "id": "5m6DtG2ec3jV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "embedding_dim = 128\n",
        "max_length = 120\n",
        "oov_tok = ''\n",
        "\n",
        "text = data['new_task']\n",
        "labels = data['y']\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=vocab_size,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    oov_token=oov_tok\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(text)\n",
        "train_sequences = tokenizer.texts_to_sequences(text)\n",
        "train_padded = pad_sequences(\n",
        "    train_sequences,\n",
        "    maxlen=max_length,\n",
        "    padding=padding_type,\n",
        "    truncating=trunc_type\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(data.new_task)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "\n",
        "for i in tqdm(range(max_length)):\n",
        "    data[f\"Tokens f.{i + 1}\"] = train_padded[:, i]"
      ],
      "metadata": {
        "id": "EO2USJjlc396"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune Bert"
      ],
      "metadata": {
        "id": "lMQqZmJzdN8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "UX9o97OfdPuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import TrainingArguments\n",
        "import torch, os\n",
        "import pandas as pd\n",
        "from transformers import pipeline, BertForSequenceClassification, BertTokenizerFast\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"f1\")\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "vHN6X43LdbEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset[['task', 'topic']]\n",
        "dataset.rename(columns={'task': 'text',\n",
        "                        'topic': 'labels'},\n",
        "               inplace=True)\n",
        "NUM_LABELS = len(dataset.labels.unique())\n",
        "\n",
        "id2label = {id: label for id, label in enumerate(dataset.labels.unique())}\n",
        "\n",
        "label2id = {label: id for id, label in enumerate(dataset.labels.unique())}\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment')\n",
        "model = BertForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment',\n",
        "                                                           num_labels=NUM_LABELS, id2label=id2label,\n",
        "                                                           label2id=label2id,\n",
        "                                                     ignore_mismatched_sizes=True)\n"
      ],
      "metadata": {
        "id": "CqbWyxwedebk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(list(X_train), truncation=True, padding=True)\n",
        "val_encodings  = tokenizer(list(X_val), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(list(X_test), truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "q0y8_cgndnmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve tokenized data for the given index\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Add the label for the given index to the item dictionary\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n"
      ],
      "metadata": {
        "id": "DZlEVxbIdpn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_encodings, list(y_train))\n",
        "val_dataloader = DataLoader(val_encodings, list(y_val))\n",
        "test_dataset = DataLoader(test_encodings, list(y_test))"
      ],
      "metadata": {
        "id": "R3biq27Odrw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataloader,\n",
        "    eval_dataset=val_dataloader,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "XsT1ZINydx2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    probs = outputs[0].softmax(1)\n",
        "    pred_label_idx = probs.argmax()\n",
        "    pred_label = model.config.id2label[pred_label_idx.item()]\n",
        "\n",
        "    return probs, pred_label_idx, pred_label\n",
        "\n",
        "\n",
        "text = input()\n",
        "predict(text)"
      ],
      "metadata": {
        "id": "lHJKZK5Wd2-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification: All Tips and Tricks from 5 Kaggle Competitions,"
      ],
      "metadata": {
        "id": "dMX7hU0qgrM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Dask –¥–ª—è —á—Ç–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: https://dask.org/\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ cuDF –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ GPU: https://docs.rapids.ai/api/cudf/stable/\n",
        "\n",
        "–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç Parquet: https://parquet.apache.org/\n",
        "\n",
        "–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç Feather: https://arrow.apache.org/docs/python/feather.html\n",
        "\n",
        "2. –ú–µ—Ç–æ–¥—ã —É–≤–µ–ª–∏—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (Data Augmentation)\n",
        "\n",
        "–ó–∞–º–µ–Ω–∞ —Å–ª–æ–≤ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28\n",
        "\n",
        "–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –≤ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è RNN: https://arxiv.org/abs/1703.02573\n",
        "\n",
        "–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –¥—Ä—É–≥–∏–µ —è–∑—ã–∫–∏ –∏ –æ–±—Ä–∞—Ç–Ω–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: https://arxiv.org/abs/1511.06709\n",
        "\n",
        "3. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤\n",
        "\n",
        "–ü—Ä–æ—Å—Ç–∞—è —Ä–∞–∑–≤–µ–¥—ã–≤–∞—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ (EDA) –¥–ª—è —Ç–≤–∏—Ç–æ–≤: https://www.kaggle.com/code/ashishpatel26/simple-eda-for-tweets\n",
        "\n",
        "EDA –¥–ª—è –¥–∞–Ω–Ω—ã—Ö Quora: https://www.kaggle.com/code/sudalairajkumar/simple-eda-for-quora-question-pairs\n",
        "\n",
        "–ü–æ–ª–Ω—ã–π EDA –¥–ª—è –¥–∞–Ω–Ω—ã—Ö Stack Exchange: https://www.kaggle.com/code/ashishpatel26/complete-eda-with-stack-exchange-data\n",
        "\n",
        "–ü—Ä–µ–¥—ã–¥—É—â–∞—è —Å—Ç–∞—Ç—å—è –∞–≤—Ç–æ—Ä–∞ –æ EDA –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞: https://neptune.ai/blog/exploratory-data-analysis-nlp\n",
        "\n",
        "4. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ TextBlob –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫: https://textblob.readthedocs.io/en/dev/\n",
        "\n",
        "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è GloVe (—á–∞—Å—Ç—å 1): https://www.kaggle.com/code/ashishpatel26/preprocessing-for-glove-part-1\n",
        "\n",
        "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è GloVe (—á–∞—Å—Ç—å 2): https://www.kaggle.com/code/ashishpatel26/preprocessing-for-glove-part-2\n",
        "\n",
        "5. –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞\n",
        "\n",
        "–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ —É–º–µ–Ω—å—à–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤: https://www.kaggle.com/code/ashishpatel26/combining-pre-trained-vectors\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Universal Sentence Encoder –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: https://tfhub.dev/google/universal-sentence-encoder/4\n",
        "\n",
        "–¢—Ä–∏ –º–µ—Ç–æ–¥–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: https://www.kaggle.com/code/ashishpatel26/3-methods-to-combine-embeddings\n",
        "\n",
        "6. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏\n",
        "\n",
        "–°—Ç–µ–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–≤—É—Ö —Å–ª–æ–µ–≤ LSTM/GRU –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: https://www.kaggle.com/code/ashishpatel26/stacking-2-layers-of-lstm-gru-networks\n",
        "\n",
        "7. –§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ–æ–∫–∞–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: https://arxiv.org/abs/1708.02002\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å \"mimic loss\", –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–∞—è –≤ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–∏ Jigsaw: https://www.kaggle.com/code/ashishpatel26/custom-mimic-loss-jigsaw\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å MTL, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–∞—è –≤ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–∏ Jigsaw: https://www.kaggle.com/code/ashishpatel26/mtl-custom-loss-jigsaw\n",
        "\n",
        "8. –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Adam —Å –ø—Ä–æ–≥—Ä–µ–≤–æ–º (warmup): https://www.kaggle.com/code/ashishpatel26/adam-with-warmup\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ BertAdam –¥–ª—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ BERT: https://www.kaggle.com/code/ashishpatel26/bert-adam\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Rectified Adam –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏: https://arxiv.org/abs/1908.03265\n",
        "\n",
        "9. –ú–µ—Ç–æ–¥—ã –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ (Callbacks)\n",
        "\n",
        "–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤: https://www.kaggle.com/code/ashishpatel26/model-checkpoint\n",
        "\n",
        "–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏: https://www.kaggle.com/code/ashishpatel26/learning-rate-scheduler\n",
        "\n",
        "–ü—Ä–æ—Å—Ç—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –æ–±—Ä–∞—Ç–Ω—ã–µ –≤—ã–∑–æ–≤—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º lambda-—Ñ—É–Ω–∫—Ü–∏–π: https://www.kaggle.com/code/ashishpatel26/simple-custom-callbacks\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞: https://www.kaggle.com/code/ashishpatel26/custom-checkpointing\n",
        "\n",
        "–°–æ–∑–¥–∞–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞—Ç–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è: https://www.kaggle.com/code/ashishpatel26/building-custom-callbacks\n",
        "\n",
        "–£–º–µ–Ω—å—à–µ–Ω–∏–µ –Ω–∞ –ø–ª–∞—Ç–æ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è, –∫–æ–≥–¥–∞ –º–µ—Ç—Ä–∏–∫–∞ –ø–µ—Ä–µ—Å—Ç–∞–µ—Ç —É–ª—É—á—à–∞—Ç—å—Å—è: https://www.kaggle.com/code/ashishpatel26/reduce-on-plateau\n",
        "\n",
        "–†–∞–Ω–Ω–µ–µ –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —É–ª—É—á—à–µ–Ω–∏–π: https://www.kaggle.com/code/ashishpatel26/early-stopping\n",
        "\n",
        "–°–Ω–∏–º–æ–∫ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ –º–æ–¥–µ–ª–∏ –≤ –æ–¥–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: https://www.kaggle.com/code/ashishpatel26/snapshot-ensembling\n",
        "\n",
        "–ë—ã—Å—Ç—Ä–æ–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: https://www.kaggle.com/code/ashishpatel26/fast-geometric-ensembling\n",
        "\n",
        "–°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤ (SWA): https://www.kaggle.com/code/ashishpatel26/stochastic-weight-averaging\n",
        "\n",
        "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è: https://www.kaggle.com/code/ashishpatel26/dynamic-learning-rate-decay\n",
        "\n",
        "10. –û—Ü–µ–Ω–∫–∞ –∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
        "\n",
        "K-–∫—Ä–∞—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "\n",
        "–°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è K-–∫—Ä–∞—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
        "\n",
        "–ì—Ä—É–ø–ø–æ–≤–∞—è K-–∫—Ä–∞—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html\n",
        "\n",
        "–ê–¥–≤–µ–Ω—Å–∞—Ä–∏–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –æ–±—É—á–∞—é—â–µ–≥–æ –∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–æ–≤: https://www.kaggle.com/code/ashishpatel26/adversarial-validation\n",
        "\n",
        "–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: https://www.kaggle.com/code/ashishpatel26/cv-analysis-different-strategies\n",
        "\n",
        "11. –¢—Ä—é–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
        "\n",
        "–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ –¥–ª–∏–Ω–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: https://www.kaggle.com/code/ashishpatel26/sequence-bucketing\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–∞ –∏ –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –µ—Å–ª–∏ –¥–ª–∏–Ω–∞ –ø—Ä–µ–≤—ã—à–∞–µ—Ç 512 —Ç–æ–∫–µ–Ω–æ–≤: https://www.kaggle.com/code/ashishpatel26/head-tail-trick\n",
        "\n",
        "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU: https://www.kaggle.com/code/ashishpatel26/use-gpu-efficiently\n",
        "\n",
        "–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ Keras: https://www.kaggle.com/code/ashishpatel26/free-keras-memory\n",
        "\n",
        "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–∞–º—è—Ç–∏: https://www.kaggle.com/code/ashishpatel26/save-load-models\n",
        "\n",
        "–ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ —Ä–µ—à–µ–Ω–∏—è—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ RNN: https://www.kaggle.com/code/ashishpatel26/dont-save-embedding-rnn\n",
        "\n",
        "–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ word2vec –±–µ–∑ –∫–ª—é—á–µ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: https://www.kaggle.com/code/ashishpatel26/load-word2vec-without-key-vectors\n",
        "\n",
        "12. –ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
        "\n",
        "–í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: https://www.kaggle.com/code/ashishpatel26/weighted-average-ensemble\n",
        "\n",
        "–°—Ç–µ–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±–æ–±—â–µ–Ω–∏–µ (stacked generalization) –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: https://www.kaggle.com/code/ashishpatel26/stacked-generalization-ensemble\n",
        "\n",
        "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–Ω–µ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ (out-of-fold predictions): https://www.kaggle.com/code/ashishpatel26/out-of-fold-predictions\n",
        "\n",
        "–°–º–µ—à–∏–≤–∞–Ω–∏–µ —Å –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π: https://www.kaggle.com/code/ashishpatel26/blending-linear-regression\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Optuna –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ —Å–º–µ—à–∏–≤–∞–Ω–∏—è: https://optuna.org/\n",
        "\n",
        "–°—Ä–µ–¥–Ω–µ–µ –ø–æ —Å—Ç–µ–ø–µ–Ω–∏ (power average) –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: https://www.kaggle.com/code/ashishpatel26/power-average-ensemble\n",
        "\n",
        "–°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–º–µ—à–∏–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç–µ–ø–µ–Ω–∏ 3.5: https://www.kaggle.com/code/ashishpatel26/power-3-5-blending-strategy"
      ],
      "metadata": {
        "id": "DWM2J2iRgslc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è"
      ],
      "metadata": {
        "id": "RiOThJijmbZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á—Ç–æ\n",
        "\n",
        "| –°—Ü–µ–Ω–∞—Ä–∏–π                                             | –ü–æ–¥—Ö–æ–¥                                         |\n",
        "| ---------------------------------------------------- | ---------------------------------------------- |\n",
        "| –ú–∞–ª–µ–Ω—å–∫–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã, —É—á–µ–±–Ω—ã–µ –∑–∞–¥–∞—á–∏                   | RNN / LSTM                                     |\n",
        "| –î–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —É–º–µ—Ä–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã        | LSTM (–¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏) –∏–ª–∏ GRU (–¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏) |\n",
        "| –¢—Ä–µ–±—É–µ—Ç—Å—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–ª–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–∞     | RNN + Attention                                |\n",
        "| –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–µ—Å—É—Ä—Å–æ–≤             | –ü–æ–ª–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤                |\n",
        "| –ë–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å, –Ω–æ –º–∞–ª–æ –ø–∞–º—è—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 16 –ì–ë GPU) | LoRA / QLoRA                                   |\n",
        "| –ù–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á –Ω–∞ –æ–¥–Ω–æ–π –±–∞–∑–µ                        | Adapters –∏–ª–∏ Prefix Tuning                     |\n",
        "| –ù–µ–±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç, few-shot –∏–ª–∏ zero-shot            | Prompt Tuning / Soft Prompts                   |"
      ],
      "metadata": {
        "id": "4pgAhrVmrj_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/purvasingh/text-generation-via-rnn-and-lstms-pytorch\n",
        "\n",
        "https://www.kaggle.com/code/neerajmohan/finetuning-large-language-models-using-qlora\n",
        "\n",
        "https://www.kaggle.com/code/thebrownviking20/intro-to-recurrent-neural-networks-lstm-gru?utm_source=chatgpt.com"
      ],
      "metadata": {
        "id": "pMX0Z369r8Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import evaluate\n",
        "import warnings\n",
        "\n",
        "# ... (previous code) ...\n",
        "\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    f1 = metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
        "    return {\n",
        "        'f1': f1[\"f1\"],\n",
        "    }\n",
        "\n",
        "# ... (rest of the code) ...\n"
      ],
      "metadata": {
        "id": "vpxXiJ88sFfF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}