{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Классификация"
      ],
      "metadata": {
        "id": "z9gxKVqJZ9dD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Библиотеки и зависимости"
      ],
      "metadata": {
        "id": "L63cLUzEaKXZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vja8TF_MRAw2"
      },
      "outputs": [],
      "source": [
        "!pip install pymorphy2\n",
        "!pip install ufal.udpipe\n",
        "!pip install wget\n",
        "!pip install gensim\n",
        "!pip install umap-learn\n",
        "!pip install datashader\n",
        "!pip install bokeh\n",
        "!pip install holoviews\n",
        "!pip install yargy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd                                   # Для работы с датасетами\n",
        "import seaborn as sns                                 # Для визуализации\n",
        "import pymorphy2 as mph                               # Для лемметизации текста\n",
        "import re                                             # Регулярные выражения\n",
        "import wget                                           # Для загрузки файлов\n",
        "import sys                                            # Для испольнения системных команд\n",
        "from gensim.models import Word2Vec as w2v             # Для использования Word2vec\n",
        "import logging                                        # Для введения логов\n",
        "import string\n",
        "import nltk\n",
        "from nltk import word_tokenize                        # Для разбиения на токены\n",
        "from nltk.corpus import stopwords                     # Для удаления стоп-слов\n",
        "import random                                         # Для перемещивания данных\n",
        "import json                                           # Для сохранения массива\n",
        "import numpy as np                                    # Для линала\n",
        "import umap                                           # Для преобразования векторов из многомерного пространство в двухмерное\n",
        "import matplotlib.pyplot as plt                       # Для графиков\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from yargy import Parser, rule, and_, or_             # Парсер\n",
        "from yargy.interpretation import fact, attribute      # Парсер\n",
        "from yargy.predicates import normalized, dictionary   # Парсер\n",
        "from yargy.pipelines import morph_pipeline            # Парсер\n",
        "from yargy.relations import main                      # Парсер\n",
        "from IPython.display import display                   # Парсер\n",
        "import spacy                                          # Парсер"
      ],
      "metadata": {
        "id": "nxYwSeyUaOLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "sw = stopwords.words('russian')"
      ],
      "metadata": {
        "id": "cUWebPm8aTn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка\n",
        "\n",
        "## 1. Предобработка текста\n",
        "\n",
        "\n",
        "* 1. ([Kaggle](https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing)).\n",
        "* 2. (https://www.kaggle.com/code/abdmental01/text-preprocessing-nlp-steps-to-process-text)).\n",
        "* 3. (https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)"
      ],
      "metadata": {
        "id": "TNjBc2VEaaet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лемматизация\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "svNVtL3eaiuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patterns = \"[A-Za-z0-9!#$%&'()*+/:;<=>?@[\\]^_`{|}~—\\\"]+\"\n",
        "morph = mph.MorphAnalyzer()\n",
        "\n",
        "def lemmatize(doc):\n",
        "    doc = re.sub(patterns, ' ', doc)\n",
        "    tokens = []\n",
        "    for token in doc.split():\n",
        "        if token:\n",
        "            token = token.strip()\n",
        "            token = morph.normal_forms(token)[0]\n",
        "            tokens.append(token)\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "TkgngBi7aZP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наташа\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9F5OUFa1ak3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_name = []\n",
        "topic_one_to_one = []\n",
        "Case = fact('Case', ['name'])\n",
        "\n",
        "def make_topic(topic: list, name: str):\n",
        "    global topic_name\n",
        "\n",
        "    topic_name.append(morph_pipeline(topic).interpretation(\n",
        "          Case.name.const(name)\n",
        "      ).interpretation(\n",
        "          Case\n",
        "      )\n",
        "    )\n",
        "\n",
        "def make_topic_one_to_one(topic: list):\n",
        "    global topic_name\n",
        "\n",
        "    return morph_pipeline(topic).interpretation(\n",
        "          Case.name.normalized()\n",
        "      ).interpretation(\n",
        "          Case\n",
        "      )\n",
        "\n",
        "top_topic = [\n",
        "    ([\"окружность\", \"угол\"], 'Геометрия'),\n",
        "\n",
        "    ([\"деление\", \"множители\"], 'Многочлен'),\n",
        "\n",
        "    ([\"клетка\", \"закрасить\"], 'Дирихле'),\n",
        "\n",
        "    ([\"делится\", \"оканчивается\"], 'Теория чисел'),\n",
        "\n",
        "    ([\"способ\", \"разделить\"], 'Комбинаторика'),\n",
        "\n",
        "    ([\"последовательность\", \"разрешаться\"], 'Инвариант'),\n",
        "\n",
        "    ([\"сумма\", \"каждый\", ], 'Оценка+Пример'),\n",
        "\n",
        "    (['город', \"ребро\",], 'Графы')\n",
        "]\n",
        "\n",
        "for name_complaint in top_topic:\n",
        "    make_topic(name_complaint[0], name_complaint[1])\n",
        "    topic_one_to_one.extend(list(name_complaint[0]))\n",
        "    for columns in list(name_complaint[0]):\n",
        "      data[columns] = np.NaN\n",
        "\n",
        "OTHERS = make_topic_one_to_one(topic_one_to_one)\n",
        "\n",
        "ALL = or_(*topic_name).interpretation(Case)\n",
        "OTHERS_ALL = or_(OTHERS).interpretation(Case)"
      ],
      "metadata": {
        "id": "_KZBPg6pbC7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%time\n",
        "parser = Parser(OTHERS_ALL)\n",
        "for ind, elem in enumerate(data['task']):\n",
        "    for match in parser.findall(str(elem)):\n",
        "        data.loc[ind, match.fact.name] = 1\n",
        "\n",
        "parser = Parser(ALL)\n",
        "for ind, elem in enumerate(data['task']):\n",
        "    for match in parser.findall(str(elem)):\n",
        "        data.loc[ind, match.fact.name] = 1"
      ],
      "metadata": {
        "id": "ddLRaC-qay5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Стоп слова"
      ],
      "metadata": {
        "id": "Mzea4BeubJv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаляем стоп-слова\n",
        "def remove_stopwords(lines, sw=sw):\n",
        "    res = []\n",
        "    for line in lines:\n",
        "        original = line\n",
        "        line = [w for w in line if w not in sw]\n",
        "        if len(line) < 1:\n",
        "            line = original\n",
        "        res.append(line)\n",
        "    return res\n",
        "\n",
        "%time filtered_lines = remove_stopwords(lines=lines, sw=sw)"
      ],
      "metadata": {
        "id": "FhyzB4ozbKoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec"
      ],
      "metadata": {
        "id": "K9T6QZfwbO5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Перемещиваем список\n",
        "random.shuffle(filtered_lines)\n",
        "# Обучаем word2vec\n",
        "%time model = w2v(filtered_lines, min_count=3, sg=1, window=7)\n",
        "\n",
        "# Сохраняем модель\n",
        "model.save(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "hxsfv6hybQn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем модель\n",
        "model = w2v.load(\"/content/drive/MyDrive/Проекты/Medsi/Models/word2vec.model\")\n",
        "\n",
        "# Производим леммитизацию колокни\n",
        "merge_data_filter_2.illness_hostory = merge_data_filter_2.illness_hostory.apply(lemmatize)\n",
        "\n",
        "# Векторизируем\n",
        "for i in range(100):\n",
        "  merge_data_filter_2[f'vector_{i}'] = 0\n",
        "\n",
        "for j, text in enumerate(merge_data_filter_2['illness_hostory']):\n",
        "  vec = np.zeros(100)\n",
        "  lens = 0\n",
        "  for word in word_tokenize(text):\n",
        "      try:\n",
        "        vec += model.wv[word]\n",
        "        lens += 1\n",
        "      except KeyError:\n",
        "        continue\n",
        "\n",
        "  vec /= lens\n",
        "  for i in range(100):\n",
        "    merge_data_filter_2.iloc[j, 103+i] = vec[i]"
      ],
      "metadata": {
        "id": "6QztLyJabVLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Umap"
      ],
      "metadata": {
        "id": "8ZaRCgLRbcpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import umap.plot"
      ],
      "metadata": {
        "id": "MTK6Cp0Dbf5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapper = umap.UMAP(densmap=True).fit(X)\n",
        "umap.plot.points(mapper)"
      ],
      "metadata": {
        "id": "9Stkupgnberf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Фильтрация пунктуации"
      ],
      "metadata": {
        "id": "JcIy_pGMb6qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "CGh5ZySab9SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Облако слов"
      ],
      "metadata": {
        "id": "pYc6eamsb-Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "y7GLBiU5cJ6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic in data.topic.unique():\n",
        "    df = data[data.topic == topic]\n",
        "    text = ' '.join(df['new_task'])\n",
        "    text_tokens = word_tokenize(text)\n",
        "\n",
        "    cloud = WordCloud(stopwords=stop_words,\n",
        "                      background_color='white').generate(' '.join(text_tokens))\n",
        "    plt.imshow(cloud)\n",
        "    plt.axis('off')\n",
        "    plt.title(topic)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lKoN4csJcFOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-граммы"
      ],
      "metadata": {
        "id": "h1ZRmWGrcOwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 30\n",
        "n = 2\n",
        "for topic in data.topic.unique():\n",
        "    df = data[data.topic == topic]\n",
        "    words = ' '.join(df.new_task_pros)\n",
        "    words = ' '.join(list(filter(lambda x: len(x) >= 2, (words.split()))))\n",
        "    tokens = nltk.word_tokenize(words)\n",
        "\n",
        "    ngrams_list = list(ngrams(tokens, n))\n",
        "    freq_dist = dict(FreqDist(ngrams_list))\n",
        "    sorted_data = sorted(freq_dist.items(), key=lambda x: -x[1])\n",
        "\n",
        "    y_labels = [str(key) for key, _ in sorted_data][:k][::-1]\n",
        "    x_values = [value for _, value in sorted_data][:k][::-1]\n",
        "\n",
        "    plt.barh(y_labels, x_values)\n",
        "    plt.xlabel('Значение')\n",
        "    plt.ylabel('Кортежи')\n",
        "    plt.title(topic)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "3-6FC_GzcQVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF"
      ],
      "metadata": {
        "id": "4IemS0spcbfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vect_tfidf(text):\n",
        "  return vectorizer.transform([text]).toarray()"
      ],
      "metadata": {
        "id": "7eYz6xlxcb4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features=5000, min_df=3)\n",
        "X = vectorizer.fit_transform(learn_tf_idf)"
      ],
      "metadata": {
        "id": "ygjAv4V7chU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenserflow token"
      ],
      "metadata": {
        "id": "5m6DtG2ec3jV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "embedding_dim = 128\n",
        "max_length = 120\n",
        "oov_tok = ''\n",
        "\n",
        "text = data['new_task']\n",
        "labels = data['y']\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=vocab_size,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    oov_token=oov_tok\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(text)\n",
        "train_sequences = tokenizer.texts_to_sequences(text)\n",
        "train_padded = pad_sequences(\n",
        "    train_sequences,\n",
        "    maxlen=max_length,\n",
        "    padding=padding_type,\n",
        "    truncating=trunc_type\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(data.new_task)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "\n",
        "for i in tqdm(range(max_length)):\n",
        "    data[f\"Tokens f.{i + 1}\"] = train_padded[:, i]"
      ],
      "metadata": {
        "id": "EO2USJjlc396"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune Bert"
      ],
      "metadata": {
        "id": "lMQqZmJzdN8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "UX9o97OfdPuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import TrainingArguments\n",
        "import torch, os\n",
        "import pandas as pd\n",
        "from transformers import pipeline, BertForSequenceClassification, BertTokenizerFast\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"f1\")\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "vHN6X43LdbEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset[['task', 'topic']]\n",
        "dataset.rename(columns={'task': 'text',\n",
        "                        'topic': 'labels'},\n",
        "               inplace=True)\n",
        "NUM_LABELS = len(dataset.labels.unique())\n",
        "\n",
        "id2label = {id: label for id, label in enumerate(dataset.labels.unique())}\n",
        "\n",
        "label2id = {label: id for id, label in enumerate(dataset.labels.unique())}\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment')\n",
        "model = BertForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment',\n",
        "                                                           num_labels=NUM_LABELS, id2label=id2label,\n",
        "                                                           label2id=label2id,\n",
        "                                                     ignore_mismatched_sizes=True)\n"
      ],
      "metadata": {
        "id": "CqbWyxwedebk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(list(X_train), truncation=True, padding=True)\n",
        "val_encodings  = tokenizer(list(X_val), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(list(X_test), truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "q0y8_cgndnmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve tokenized data for the given index\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Add the label for the given index to the item dictionary\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n"
      ],
      "metadata": {
        "id": "DZlEVxbIdpn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_encodings, list(y_train))\n",
        "val_dataloader = DataLoader(val_encodings, list(y_val))\n",
        "test_dataset = DataLoader(test_encodings, list(y_test))"
      ],
      "metadata": {
        "id": "R3biq27Odrw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataloader,\n",
        "    eval_dataset=val_dataloader,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "XsT1ZINydx2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    probs = outputs[0].softmax(1)\n",
        "    pred_label_idx = probs.argmax()\n",
        "    pred_label = model.config.id2label[pred_label_idx.item()]\n",
        "\n",
        "    return probs, pred_label_idx, pred_label\n",
        "\n",
        "\n",
        "text = input()\n",
        "predict(text)"
      ],
      "metadata": {
        "id": "lHJKZK5Wd2-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification: All Tips and Tricks from 5 Kaggle Competitions,"
      ],
      "metadata": {
        "id": "dMX7hU0qgrM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Оптимизация памяти при работе с большими датасетами\n",
        "\n",
        "Использование Dask для чтения и обработки данных: https://dask.org/\n",
        "\n",
        "Использование cuDF для ускоренной обработки данных на GPU: https://docs.rapids.ai/api/cudf/stable/\n",
        "\n",
        "Конвертация данных в формат Parquet: https://parquet.apache.org/\n",
        "\n",
        "Конвертация данных в формат Feather: https://arrow.apache.org/docs/python/feather.html\n",
        "\n",
        "2. Методы увеличения данных (Data Augmentation)\n",
        "\n",
        "Замена слов синонимами для увеличения данных: https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28\n",
        "\n",
        "Добавление шума в тексты для обучения RNN: https://arxiv.org/abs/1703.02573\n",
        "\n",
        "Перевод текста на другие языки и обратно для создания новых примеров: https://arxiv.org/abs/1511.06709\n",
        "\n",
        "3. Исследование данных и получение инсайтов\n",
        "\n",
        "Простая разведывательная аналитика (EDA) для твитов: https://www.kaggle.com/code/ashishpatel26/simple-eda-for-tweets\n",
        "\n",
        "EDA для данных Quora: https://www.kaggle.com/code/sudalairajkumar/simple-eda-for-quora-question-pairs\n",
        "\n",
        "Полный EDA для данных Stack Exchange: https://www.kaggle.com/code/ashishpatel26/complete-eda-with-stack-exchange-data\n",
        "\n",
        "Предыдущая статья автора о EDA для обработки естественного языка: https://neptune.ai/blog/exploratory-data-analysis-nlp\n",
        "\n",
        "4. Очистка данных\n",
        "\n",
        "Использование TextBlob для исправления орфографических ошибок: https://textblob.readthedocs.io/en/dev/\n",
        "\n",
        "Предобработка для GloVe (часть 1): https://www.kaggle.com/code/ashishpatel26/preprocessing-for-glove-part-1\n",
        "\n",
        "Предобработка для GloVe (часть 2): https://www.kaggle.com/code/ashishpatel26/preprocessing-for-glove-part-2\n",
        "\n",
        "5. Представление текста\n",
        "\n",
        "Комбинирование предварительно обученных векторов для лучшего представления текста и уменьшения количества неизвестных слов: https://www.kaggle.com/code/ashishpatel26/combining-pre-trained-vectors\n",
        "\n",
        "Использование Universal Sentence Encoder для генерации признаков на уровне предложений: https://tfhub.dev/google/universal-sentence-encoder/4\n",
        "\n",
        "Три метода комбинирования эмбеддингов: https://www.kaggle.com/code/ashishpatel26/3-methods-to-combine-embeddings\n",
        "\n",
        "6. Архитектура модели\n",
        "\n",
        "Стекирование двух слоев LSTM/GRU для улучшения производительности: https://www.kaggle.com/code/ashishpatel26/stacking-2-layers-of-lstm-gru-networks\n",
        "\n",
        "7. Функции потерь\n",
        "\n",
        "Использование фокальной функции потерь для несбалансированных данных: https://arxiv.org/abs/1708.02002\n",
        "\n",
        "Пользовательская функция потерь \"mimic loss\", использованная в соревновании Jigsaw: https://www.kaggle.com/code/ashishpatel26/custom-mimic-loss-jigsaw\n",
        "\n",
        "Пользовательская функция потерь MTL, использованная в соревновании Jigsaw: https://www.kaggle.com/code/ashishpatel26/mtl-custom-loss-jigsaw\n",
        "\n",
        "8. Оптимизаторы\n",
        "\n",
        "Использование Adam с прогревом (warmup): https://www.kaggle.com/code/ashishpatel26/adam-with-warmup\n",
        "\n",
        "Использование BertAdam для моделей на основе BERT: https://www.kaggle.com/code/ashishpatel26/bert-adam\n",
        "\n",
        "Использование Rectified Adam для стабилизации обучения и ускорения сходимости: https://arxiv.org/abs/1908.03265\n",
        "\n",
        "9. Методы обратного вызова (Callbacks)\n",
        "\n",
        "Контрольная точка модели для мониторинга и сохранения весов: https://www.kaggle.com/code/ashishpatel26/model-checkpoint\n",
        "\n",
        "Планировщик скорости обучения для изменения скорости обучения на основе производительности модели: https://www.kaggle.com/code/ashishpatel26/learning-rate-scheduler\n",
        "\n",
        "Простые пользовательские обратные вызовы с использованием lambda-функций: https://www.kaggle.com/code/ashishpatel26/simple-custom-callbacks\n",
        "\n",
        "Пользовательская контрольная точка: https://www.kaggle.com/code/ashishpatel26/custom-checkpointing\n",
        "\n",
        "Создание собственных обратных вызовов для различных случаев использования: https://www.kaggle.com/code/ashishpatel26/building-custom-callbacks\n",
        "\n",
        "Уменьшение на плато для снижения скорости обучения, когда метрика перестает улучшаться: https://www.kaggle.com/code/ashishpatel26/reduce-on-plateau\n",
        "\n",
        "Раннее прекращение обучения при отсутствии улучшений: https://www.kaggle.com/code/ashishpatel26/early-stopping\n",
        "\n",
        "Снимок ансамблирования для получения различных контрольных точек модели в одном обучении: https://www.kaggle.com/code/ashishpatel26/snapshot-ensembling\n",
        "\n",
        "Быстрое геометрическое ансамблирование: https://www.kaggle.com/code/ashishpatel26/fast-geometric-ensembling\n",
        "\n",
        "Стохастическое усреднение весов (SWA): https://www.kaggle.com/code/ashishpatel26/stochastic-weight-averaging\n",
        "\n",
        "Динамическое уменьшение скорости обучения: https://www.kaggle.com/code/ashishpatel26/dynamic-learning-rate-decay\n",
        "\n",
        "10. Оценка и кросс-валидация\n",
        "\n",
        "K-кратная кросс-валидация: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "\n",
        "Стратифицированная K-кратная кросс-валидация: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
        "\n",
        "Групповая K-кратная кросс-валидация: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html\n",
        "\n",
        "Адвенсариальная валидация для проверки сходства распределений обучающего и тестового наборов: https://www.kaggle.com/code/ashishpatel26/adversarial-validation\n",
        "\n",
        "Анализ различных стратегий кросс-валидации: https://www.kaggle.com/code/ashishpatel26/cv-analysis-different-strategies\n",
        "\n",
        "11. Трюки для ускорения выполнения\n",
        "\n",
        "Сортировка последовательностей по длине для экономии времени выполнения и улучшения производительности: https://www.kaggle.com/code/ashishpatel26/sequence-bucketing\n",
        "\n",
        "Использование только начала и конца предложений, если длина превышает 512 токенов: https://www.kaggle.com/code/ashishpatel26/head-tail-trick\n",
        "\n",
        "Эффективное использование GPU: https://www.kaggle.com/code/ashishpatel26/use-gpu-efficiently\n",
        "\n",
        "Очистка памяти Keras: https://www.kaggle.com/code/ashishpatel26/free-keras-memory\n",
        "\n",
        "Сохранение и загрузка моделей для экономии времени и памяти: https://www.kaggle.com/code/ashishpatel26/save-load-models\n",
        "\n",
        "Не сохранять эмбеддинги в решениях на основе RNN: https://www.kaggle.com/code/ashishpatel26/dont-save-embedding-rnn\n",
        "\n",
        "Загрузка векторов word2vec без ключевых векторов: https://www.kaggle.com/code/ashishpatel26/load-word2vec-without-key-vectors\n",
        "\n",
        "12. Ансамблирование моделей\n",
        "\n",
        "Взвешенное среднее ансамблирование: https://www.kaggle.com/code/ashishpatel26/weighted-average-ensemble\n",
        "\n",
        "Стекированное обобщение (stacked generalization) ансамблирование: https://www.kaggle.com/code/ashishpatel26/stacked-generalization-ensemble\n",
        "\n",
        "Предсказания вне обучающего набора (out-of-fold predictions): https://www.kaggle.com/code/ashishpatel26/out-of-fold-predictions\n",
        "\n",
        "Смешивание с линейной регрессией: https://www.kaggle.com/code/ashishpatel26/blending-linear-regression\n",
        "\n",
        "Использование Optuna для определения весов смешивания: https://optuna.org/\n",
        "\n",
        "Среднее по степени (power average) ансамблирование: https://www.kaggle.com/code/ashishpatel26/power-average-ensemble\n",
        "\n",
        "Стратегия смешивания с использованием степени 3.5: https://www.kaggle.com/code/ashishpatel26/power-3-5-blending-strategy"
      ],
      "metadata": {
        "id": "DWM2J2iRgslc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Генерация"
      ],
      "metadata": {
        "id": "RiOThJijmbZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Когда использовать что\n",
        "\n",
        "| Сценарий                                             | Подход                                         |\n",
        "| ---------------------------------------------------- | ---------------------------------------------- |\n",
        "| Маленькие датасеты, учебные задачи                   | RNN / LSTM                                     |\n",
        "| Длинные последовательности, умеренные ресурсы        | LSTM (для стабильности) или GRU (для скорости) |\n",
        "| Требуется копирование или внимание к части входа     | RNN + Attention                                |\n",
        "| Лучшее качество, много данных и ресурсов             | Полное дообучение трансформеров                |\n",
        "| Большая модель, но мало памяти (например, 16 ГБ GPU) | LoRA / QLoRA                                   |\n",
        "| Несколько задач на одной базе                        | Adapters или Prefix Tuning                     |\n",
        "| Небольшой датасет, few-shot или zero-shot            | Prompt Tuning / Soft Prompts                   |"
      ],
      "metadata": {
        "id": "4pgAhrVmrj_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/purvasingh/text-generation-via-rnn-and-lstms-pytorch\n",
        "\n",
        "https://www.kaggle.com/code/neerajmohan/finetuning-large-language-models-using-qlora\n",
        "\n",
        "https://www.kaggle.com/code/thebrownviking20/intro-to-recurrent-neural-networks-lstm-gru?utm_source=chatgpt.com"
      ],
      "metadata": {
        "id": "pMX0Z369r8Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import evaluate\n",
        "import warnings\n",
        "\n",
        "# ... (previous code) ...\n",
        "\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    f1 = metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
        "    return {\n",
        "        'f1': f1[\"f1\"],\n",
        "    }\n",
        "\n",
        "# ... (rest of the code) ...\n"
      ],
      "metadata": {
        "id": "vpxXiJ88sFfF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}